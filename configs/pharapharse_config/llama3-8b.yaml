#Training args
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
torch_dtype: bfloat16
use_lora: false
quantization: null
predict_with_generate: true
do_predict: true
per_device_eval_batch_size: 1
use_flash_attention: true
deepspeed: configs/deepspeed_configs/deepspeed_zero3.json

generation_args_json: configs/pharapharse_config/generation_config.json
output_dir: paraphrase/Meta-Llama-3-8B-Instruct


# dataset arguments
datasets:
  - ace05
  - rams
  - conll03
  - casie
  - tacred
  - ontonotes5
  - ncbidisease
  - bc5cdr
  - diann
  - wnut17
  - multinerd
  - wikievents
  - fabner
  - e3c
  - broadtwitter
  - harveyner
  - mitmovie
  - mitrestaurant
  - crossner

language: en

# reporting
logging_strategy: steps
logging_first_step: true
logging_steps: 25
report_to: none


# hub settings
push_to_hub: false
resume_from_checkpoint: false

# performance
bf16: true
fp16: false
torch_compile: false
ddp_find_unused_parameters: false